# ğŸ¨ Image Data Augmentation with TensorFlow

<div align="center">

![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)


**Enhancing MNIST Classification through Smart Data Augmentation**

[Overview](#-overview) â€¢ [Features](#-features) â€¢ [Results](#-results) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage)

</div>

---

## ğŸ¯ Overview

This project demonstrates the **power of image data augmentation** in improving model generalization and reducing overfitting. Using TensorFlow's powerful image augmentation APIs, we apply multiple transformations dynamically during model training on the classic MNIST handwritten digits dataset.

### Key Highlights

- âœ¨ Real-time augmentation using `tf.data` pipelines
- ğŸš€ Improved validation accuracy from **~94%** to **~95%**
- ğŸ”„ Multiple augmentation techniques combined
- ğŸ“Š Comprehensive visualization of results

---

## ğŸ› ï¸ Features

### Augmentation Techniques

Our pipeline implements the following transformations on-the-fly:

| Technique | Description |
|-----------|-------------|
| ğŸ”„ **Horizontal Flipping** | Mirrors images horizontally |
| âš« **Grayscale Conversion** | Converts to single-channel representation |
| ğŸ’¡ **Brightness Adjustment** | Randomly adjusts image brightness |
| ğŸŒˆ **Saturation Adjustment** | Modifies color intensity |
| ğŸ”ƒ **Rotation** | Rotates images at various angles |
| âœ‚ï¸ **Random Cropping** | Crops with padding for variety |
| ğŸ² **Random Brightness** | Stochastic brightness changes |

All augmentations are applied **dynamically** during training using efficient `tf.data` pipelines for optimal performance.

---

## ğŸ“Š Results

### Performance Comparison

| Model Configuration | Validation Accuracy | Improvement |
|---------------------|---------------------|-------------|
| âŒ Without Augmentation | ~94% | Baseline |
| âœ… With Augmentation | **~95%** | **+1%** |

> **Note:** Data augmentation successfully improved generalization and reduced overfitting, though convergence was slightly slower due to increased data diversity.

### Visual Results

#### Augmentation Examples
![Augmentation Examples](results/augmentation_examples.png)

*Various augmentation techniques applied to MNIST digits*

#### Accuracy Comparison
![Accuracy Comparison](results/accuracy_comparison.png)

*Training and validation accuracy with and without augmentation*

---

## ğŸ“¦ Dataset

**MNIST Handwritten Digits**

- ğŸ“ Training subset: **2,048 samples**
- âœ… Validation: **Full test set** (10,000 samples)
- ğŸ–¼ï¸ Image size: 28Ã—28 grayscale
- ğŸ”¢ Classes: 10 (digits 0-9)

---

## ğŸ—ï¸ Model Architecture

### Fully Connected Neural Network

```
Input Layer (784 neurons)
    â†“
Dense Layer (4096 units, ReLU)
    â†“
Dense Layer (4096 units, ReLU)
    â†“
Output Layer (10 units, Softmax)
```

**Training Configuration:**
- ğŸ¯ Optimizer: **Adam**
- ğŸ“‰ Loss: **Sparse Categorical Crossentropy**
- ğŸ”„ Activation: **ReLU** (hidden layers)
- ğŸ“Š Metrics: **Accuracy**

---

## ğŸš€ Installation

### Prerequisites

- Python 3.7+
- pip package manager

### Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/mnist-augmentation.git
   cd mnist-augmentation
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

### Dependencies

```txt
tensorflow>=2.8.0
numpy>=1.21.0
matplotlib>=3.4.0
```

---

## ğŸ’» Usage

### Training the Model

Run the training script with augmentation:

```bash
python src/train.py
```

### Configuration Options

Customize augmentation parameters in `src/config.py`:

```python
AUGMENTATION_CONFIG = {
    'flip_horizontal': True,
    'rotation_range': 15,
    'brightness_delta': 0.2,
    'saturation_range': (0.8, 1.2)
}
```

### Project Structure

```
mnist-augmentation/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py          # Main training script
â”‚   â”œâ”€â”€ augmentation.py   # Augmentation functions
â”‚   â””â”€â”€ model.py          # Model architecture
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ augmentation_examples.png
â”‚   â””â”€â”€ accuracy_comparison.png
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“ˆ Key Insights

1. **Improved Generalization**: Augmentation reduced overfitting by exposing the model to more diverse training samples
2. **Trade-off**: Slightly slower convergence due to increased data complexity
3. **Efficiency**: On-the-fly augmentation using `tf.data` maintains training speed
4. **Robustness**: Model becomes more robust to variations in test data

---

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:

- ğŸ› Report bugs
- ğŸ’¡ Suggest new augmentation techniques
- ğŸ”§ Submit pull requests
- â­ Star this repository

---



---

## ğŸ™ Acknowledgments

- TensorFlow team for excellent documentation
- MNIST dataset creators
- Open source community

---

<div align="center">

**Made with â¤ï¸ and TensorFlow**

[â¬† Back to Top](#-image-data-augmentation-with-tensorflow)

</div>
